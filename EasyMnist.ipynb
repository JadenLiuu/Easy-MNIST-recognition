{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Acc of val data: 0.26544174551963806 at iter 0\n",
      "Acc of val data: 0.9272869229316711 at iter 50\n",
      "Acc of val data: 0.9429241418838501 at iter 100\n",
      "Acc of val data: 0.9476153254508972 at iter 150\n",
      "Acc of val data: 0.9534792900085449 at iter 200\n",
      "Acc of val data: 0.9566067457199097 at iter 250\n",
      "Acc of val data: 0.9632525444030762 at iter 300\n",
      "Acc of val data: 0.9675527811050415 at iter 350\n",
      "Acc of val data: 0.961297869682312 at iter 400\n",
      "early stop at iteration 400\n",
      "Acc of test data: 0.9708114266395569\n",
      "mAP of test data: 0.9683457847848265\n",
      "Recall of test data: 0.9706112350284363\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "#### number of classes (5 here, due to 0~4), setting of using dropout\n",
    "num_cls = 5\n",
    "using_dropout = True # to do bonus2 or not\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "This function compute accuracy of `x_data` by checking the `y_label`\n",
    "\"\"\"    \n",
    "def acc(x_data, y_label):\n",
    "    global prediction\n",
    "    \n",
    "    y_hat = sess.run(prediction, {X0: x_data})\n",
    "    corr = tf.cast(tf.equal(tf.argmax(y_hat, 1), y_label), tf.float32)\n",
    "    acc = tf.reduce_mean(corr)\n",
    "    \n",
    "    result = sess.run(acc, {X0: x_data, Y0: y_label})\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "AP is a sub-function used by function mAP, \n",
    "which is decided to compute the average precision of each input class `cls_k`\n",
    "\"\"\"\n",
    "def AP(output, target, cls_k):\n",
    "    # sort predictions\n",
    "    sort_ind = np.argsort(output) # sort the prediction to justify the retrieval performance   \n",
    "    sort_ind = sort_ind[::-1]    \n",
    "        \n",
    "    pos = 0.0\n",
    "    total_count = 0.0\n",
    "    precision_at_ind = 0.0\n",
    "    for ind in sort_ind:\n",
    "        label = target[ind]\n",
    "        total_count += 1\n",
    "        if label == cls_k:\n",
    "            pos += 1\n",
    "            precision_at_ind += pos/total_count\n",
    "    \n",
    "    if pos == 0.0:\n",
    "        return precision_at_ind\n",
    "    \n",
    "    return precision_at_ind / pos\n",
    "    \n",
    "\"\"\"\n",
    "By averaging the mean of AP of each classes,\n",
    "this function returns the mAP of `x_data`\n",
    "\"\"\"\n",
    "def mAP(x_data, y_label):\n",
    "    ap = np.zeros(5)\n",
    "    \n",
    "    y_hat = sess.run(prediction, {X0: x_data}) # shape: 2558, 5 ; ndarray    \n",
    "        \n",
    "    for k in range(num_cls):\n",
    "        output = y_hat[:, k]                \n",
    "        ap[k] = AP(output, y_label, k)                \n",
    "            \n",
    "    mAP = tf.reduce_mean(tf.convert_to_tensor(ap))\n",
    "    result = sess.run(mAP, {X0: x_data, Y0: y_label})\n",
    "    return result\n",
    "\n",
    "\"\"\"\n",
    "This function computes recall by computing TP/(TP+FN)\n",
    "\"\"\"    \n",
    "def recall(x_data, y_label):\n",
    "    tmp = np.zeros(5)\n",
    "    y_hat = sess.run(prediction, {X0: x_data}) # shape: 2558, 5 ; ndarray    \n",
    "    predict_label = np.argmax(y_hat, axis=1)    \n",
    "        \n",
    "    for k in range(num_cls):        \n",
    "        label_k = (y_label==k) # bool\n",
    "        TP_FN = sum(label_k.astype(int)) # number of label k in `y_label`        \n",
    "        predict_k = np.logical_and((predict_label==k),label_k)        \n",
    "        TP = sum(predict_k.astype(int)) # number of True positive\n",
    "        \n",
    "        tmp[k] = TP/TP_FN\n",
    "        #print(tmp[k])\n",
    "    \n",
    "    recall = tf.reduce_mean(tf.convert_to_tensor(tmp))\n",
    "    result = sess.run(recall, {X0: x_data, Y0: y_label})\n",
    "    return result\n",
    "    \n",
    "\n",
    "# define variable function\n",
    "def weight_var(name, shape):\n",
    "    #normal = tf.Variable(tf.truncated_normal(shape, stddev=0.1, dtype= tf.float64))\n",
    "    var = tf.get_variable(name, shape=shape, dtype=tf.float64, initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "    return var\n",
    "\n",
    "def bias_var(shape):\n",
    "    return tf.Variable(tf.constant(0.1, dtype=tf.float64, shape=shape))\n",
    "\n",
    "def shuffle_aligned_list(data):\n",
    "    \"\"\"Shuffle arrays in a list by shuffling each array identically.\"\"\"\n",
    "    num = data[0].shape[0]\n",
    "    p = np.random.permutation(num)\n",
    "    return [d[p] for d in data]\n",
    "\n",
    "\n",
    "def batch_generator(data, batch_size, shuffle=True):\n",
    "    \"\"\"Generate batches of data.\n",
    "\n",
    "    Given a list of array-like objects, generate batches of a given\n",
    "    size by yielding a list of array-like objects corresponding to the\n",
    "    same slice of each input.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        data = shuffle_aligned_list(data)\n",
    "\n",
    "    batch_count = 0\n",
    "    while True:\n",
    "        if batch_count * batch_size + batch_size >= len(data[0]):\n",
    "            batch_count = 0\n",
    "\n",
    "            if shuffle:\n",
    "                data = shuffle_aligned_list(data)\n",
    "\n",
    "        start = batch_count * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_count += 1\n",
    "        yield [d[start:end] for d in data]\n",
    "        \n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# training on MNIST but only on digits 0 to 4\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5]\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5]\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5]\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------Network START\n",
    "# define placeholder\n",
    "X0 = tf.placeholder(tf.float64, [None, 784])\n",
    "Y0 = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# define layer 1\n",
    "w1 = weight_var(\"w1\",[784,128])\n",
    "L1_output = tf.nn.elu(tf.matmul(X0,w1)+bias_var([128]))\n",
    "\n",
    "# define layer 2\n",
    "w2 = weight_var(\"w2\",[128,128])\n",
    "L2_output = tf.nn.elu(tf.matmul(L1_output,w2)+bias_var([128]))\n",
    "if using_dropout:\n",
    "    L2_output = tf.nn.dropout(L2_output, 0.5)\n",
    "\n",
    "# define layer 3\n",
    "w3 = weight_var(\"w3\",[128,128])\n",
    "L3_output = tf.nn.elu(tf.matmul(L2_output,w3)+bias_var([128]))\n",
    "if using_dropout:\n",
    "    L3_output = tf.nn.dropout(L3_output, 0.5)\n",
    "\n",
    "# define layer 4\n",
    "w4 = weight_var(\"w4\",[128,128])\n",
    "L4_output = tf.nn.elu(tf.matmul(L3_output,w4)+bias_var([128]))\n",
    "if using_dropout:\n",
    "    L4_output = tf.nn.dropout(L4_output, 0.5)\n",
    "\n",
    "# define layer 5\n",
    "w5 = weight_var(\"w5\",[128,128])\n",
    "L5_output = tf.nn.elu(tf.matmul(L4_output,w5)+bias_var([128]))\n",
    "#if using_dropout:\n",
    "#    L5_output = tf.nn.dropout(L5_output, 0.5)\n",
    "\n",
    "# define softmax layer\n",
    "w_predict = weight_var(\"w_predict\",[128,num_cls])\n",
    "prediction = tf.matmul(L5_output,w_predict)+bias_var([num_cls])\n",
    "#------------------------------------------------------------------------------------Network END\n",
    "\n",
    "\n",
    "# error & optimizer\n",
    "error = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=prediction, labels=Y0)\n",
    "loss = tf.reduce_mean(error)\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# session\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "if not using_dropout:\n",
    "    if not os.path.exists('net'):\n",
    "        os.makedirs('net')\n",
    "    check_path = \"net/Team36_HW2.ckpt\"\n",
    "else:\n",
    "    if not os.path.exists('net_drop'):\n",
    "        os.makedirs('net_drop')\n",
    "    check_path = \"net_drop/Team36_HW2.ckpt\"\n",
    "\n",
    "# batch setting     \n",
    "batch_size = 500\n",
    "generate_batch = batch_generator([X_train1, y_train1], batch_size, shuffle=True)\n",
    "# training \n",
    "last_score = 0.0\n",
    "for i in range(10000):\n",
    "    x0, y0 = next(generate_batch)\n",
    "    sess.run(train_step, feed_dict={X0: x0, Y0: y0})        \n",
    "    if i%50 == 0:\n",
    "        val_result = acc(X_valid1, y_valid1)\n",
    "        print('Acc of val data: {} at iter {iter}'.format(val_result, iter=i))\n",
    "        if val_result > last_score:                \n",
    "            if os.path.exists(check_path): ## remove last saved model\n",
    "                os.remove(check_path)                           \n",
    "            saver.save(sess, check_path) ## # save the current model to check path                 \n",
    "            last_score = val_result # update last score\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "\n",
    "            \n",
    "using_dropout = False  ## Not using dropout during testing\n",
    "print('early stop at iteration %d' % i)\n",
    "print('Acc of test data: {}'.format(acc(X_test1, y_test1)))\n",
    "print('mAP of test data: {}'.format(mAP(X_test1, y_test1)))\n",
    "print('Recall of test data: {}'.format(recall(X_test1, y_test1)))  \n",
    "\n",
    "#=============================================Training process====================================================\n",
    "#1. Define architecture of this network : [fc-784 to 128]+[fc-128 to 128]*4+[fc-128 to 5]\n",
    "#    - by calling sub-function weight_var and bias_var to produce Variables.\n",
    "#    - the initializer of weight variables are `tf.contrib.layers.variance_scaling_initializer()`\n",
    "#2. compute error by using `sparse_softmax_cross_entropy_with_logits`\n",
    "#3. Set optimizer with Adam, and set the initial learning rate to 0.001\n",
    "#4. run session to train this network\n",
    "#    - for every 50 epochs, we test the network on validation set\n",
    "#    - In comparison of last 50 epochs, if the score of validation set becomes worse, we do a early stop,\n",
    "#        otherwise, we remove the net we stored before, and save the current model to net/net_drop directory.\n",
    "#5. [Bonus2] by setting `using_dropout` at line 8, this network could perform dropout at 2nd&3rd&4th fc layers\n",
    "#    - default setting of `using_dropout` is True\n",
    "\n",
    "#---------------------- base result ----------------------\n",
    "\n",
    "#early stop at iteration 300\n",
    "#Acc of test data: 0.9916326403617859\n",
    "#mAP of test data: 0.9961790667255332\n",
    "#Recall of test data: 0.991575376527425\n",
    "\n",
    "\n",
    "#---------------------- base result + bonus 2-------------\n",
    "\n",
    "#early stop at iteration 200\n",
    "#Acc of test data: 0.9708114266395569\n",
    "#mAP of test data: 0.9683457847848265\n",
    "#Recall of test data: 0.9706112350284363\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
